{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib.machinery\n",
    "es = importlib.machinery.SourceFileLoader('extrasense','/home/sac086/extrasensory/extrasense/extrasense.py').load_module()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating Exp1\n",
    "\n",
    "**Experimental Conditions**: Impersonal, Personal, Hybrid training data\n",
    "\n",
    "**Experiment Output**: User ID, Method, Run number, Accuracy\n",
    "\n",
    "**Sampling Method for personal data**: randomly sample the data, no stratification of sample classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with and without stratification for one participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = importlib.machinery.SourceFileLoader('extrasense','/home/sac086/extrasensory/processes/experimental_setups.py').load_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for 0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "getting impersonal data...\n",
      "Scaling Impersonal...\n",
      "Initializing ShuffleSplit()\n",
      "Starting run #1\n",
      "\tRun #1\n",
      "\tpersonal : 0.63\n",
      "\thybrid : 0.7\n",
      "\timpersonal : 0.75\n",
      "\n",
      "\n",
      "Starting run #2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d65e339c02b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0muser_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/extrasensory/extrasense/extrasense.py\u001b[0m in \u001b[0;36mrun_experiment1\u001b[0;34m(user_id, training_size, learning_algo, stratified)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# build and predict hybrid model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mhybrid_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mhybrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_hybrid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybrid_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mhybrid_scaled_val_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhybrid_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mhybrid_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhybrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhybrid_scaled_val_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 327\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_sizes = [5,10,20,30,40,50,60]\n",
    "user_id = es.user_ids[3]\n",
    "\n",
    "rows = []\n",
    "for ts in training_sizes:\n",
    "    print(\"Getting scores for %s\" % user_id)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        user_rows = exp.run_experiment1(user_id, training_size=ts)\n",
    "    except ValueError as ve:\n",
    "        errors.append(ve)\n",
    "        continue\n",
    "    finish = time.time()\n",
    "    duration_in_minutes = (finish - start) / 60.\n",
    "    print(\"\\ttook %s minutes\" % (duration_in_minutes))\n",
    "    rows += user_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for 0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "getting impersonal data...\n",
      "Scaling Impersonal...\n",
      "Initializing StratifiedShuffleSplit()\n",
      "Starting run #1\n",
      "\tRun #1\n",
      "\tpersonal : 0.53\n",
      "\thybrid : 0.6\n",
      "\timpersonal : 0.6\n",
      "\n",
      "\n",
      "Starting run #2\n",
      "\tRun #2\n",
      "\tpersonal : 0.46\n",
      "\thybrid : 0.59\n",
      "\timpersonal : 0.6\n",
      "\n",
      "\n",
      "Starting run #3\n",
      "\tRun #3\n",
      "\tpersonal : 0.22\n",
      "\thybrid : 0.63\n",
      "\timpersonal : 0.61\n",
      "\n",
      "\n",
      "Starting run #4\n",
      "\tRun #4\n",
      "\tpersonal : 0.62\n",
      "\thybrid : 0.64\n",
      "\timpersonal : 0.68\n",
      "\n",
      "\n",
      "Starting run #5\n",
      "\tRun #5\n",
      "\tpersonal : 0.52\n",
      "\thybrid : 0.65\n",
      "\timpersonal : 0.67\n",
      "\n",
      "\n",
      "\ttook 25.76958030462265 minutes\n",
      "Getting scores for 0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "getting impersonal data...\n",
      "Scaling Impersonal...\n",
      "Initializing StratifiedShuffleSplit()\n",
      "Starting run #1\n",
      "\tRun #1\n",
      "\tpersonal : 0.71\n",
      "\thybrid : 0.69\n",
      "\timpersonal : 0.67\n",
      "\n",
      "\n",
      "Starting run #2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5f55a715cb3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0muser_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/extrasensory/extrasense/extrasense.py\u001b[0m in \u001b[0;36mrun_experiment1\u001b[0;34m(user_id, training_size, learning_algo, stratified)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# build and predict hybrid model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mhybrid_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mhybrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_hybrid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybrid_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mhybrid_scaled_val_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhybrid_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mhybrid_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhybrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhybrid_scaled_val_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 327\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_sizes = [5,10,20,30,40,50,60]\n",
    "user_id = es.user_ids[3]\n",
    "\n",
    "rows_stratified = []\n",
    "for ts in training_sizes:\n",
    "    print(\"Getting scores for %s\" % user_id)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        user_rows = exp.run_experiment1(user_id, training_size=ts, stratified=True)\n",
    "    except ValueError as ve:\n",
    "        errors.append(ve)\n",
    "        continue\n",
    "    finish = time.time()\n",
    "    duration_in_minutes = (finish - start) / 60.\n",
    "    print(\"\\ttook %s minutes\" % (duration_in_minutes))\n",
    "    rows_stratified += user_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview.block=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview.scatter('user_ids', es.user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_string = '''import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib.machinery\n",
    "es = importlib.machinery.SourceFileLoader('extrasense','/home/sac086/extrasensory/extrasense/extrasense.py').load_module()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "asr = dview.execute(import_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def run_experiment1(user_id, training_size=100, learning_algo=es.weka_RF, stratified=False):\n",
    "    #print(\"Getting personal data...\")\n",
    "    personal_df = es.get_data_from_user_id(user_id, data_type=\"activity\", labeled_only=True)\n",
    "    timestamps_personal = personal_df.pop('timestamp')\n",
    "    label_sources_personal = personal_df.pop(\"label_source\")\n",
    "    personal_labels = personal_df.pop(\"label\")\n",
    "    \n",
    "    # train impersonal model\n",
    "    print(\"getting impersonal data...\")\n",
    "    impersonal_df = es.get_impersonal_data(leave_users_out=[user_id], data_type=\"activity\", labeled_only=True)\n",
    "    timestamps_impersonal = impersonal_df.pop('timestamp')\n",
    "    impersonal_train_labels = impersonal_df.pop(\"label\")\n",
    "    impersonal_label_sources = impersonal_df.pop('label_source')\n",
    "    impersonal_ids = impersonal_df.pop(\"user_id\")\n",
    "    #impersonal_df.dropna()\n",
    "    \n",
    "    #impersonal_train_labels = impersonal_train_labels.index[impersonal_df.index]\n",
    "    \n",
    "     # standard scale training\n",
    "    print(\"Scaling Impersonal...\")\n",
    "    impersonal_scaler = StandardScaler().fit(impersonal_df)\n",
    "    scaled_impersonal_df = impersonal_scaler.transform(impersonal_df)\n",
    "    \n",
    "    impersonal_clf = learning_algo()\n",
    "    impersonal_clf.fit(scaled_impersonal_df, impersonal_train_labels)\n",
    "    \n",
    "    # setup sampler\n",
    "    if stratified:\n",
    "        rs = StratifiedShuffleSplit(n_splits=5, train_size=training_size, test_size=100)\n",
    "        splitter =  rs.split(personal_df, personal_labels)\n",
    "    else:\n",
    "        rs = ShuffleSplit(n_splits=5, train_size=training_size, test_size=100)\n",
    "        splitter = rs.split(personal_df)\n",
    "    \n",
    "    run_count = 1\n",
    "    rows = []\n",
    "    # run sampler\n",
    "    for train_ind, test_ind in splitter:\n",
    "        print(\"Starting run #%s\" % run_count)\n",
    "        personal_train_df = personal_df.iloc[train_ind]\n",
    "        personal_train_labels = personal_labels.iloc[train_ind]\n",
    "        \n",
    "        val_df = personal_df.iloc[test_ind]\n",
    "        val_labels = personal_labels.iloc[test_ind]\n",
    "        \n",
    "        \n",
    "        #return personal_train_df, personal_train_labels, impersonal_df, impersonal_train_labels\n",
    "        hybrid_train_df = pd.concat([impersonal_df, personal_train_df])\n",
    "        #return impersonal_train_labels, personal_train_labels\n",
    "        hybrid_train_labels = pd.concat([impersonal_train_labels, personal_train_labels])\n",
    "        \n",
    "        # scale \n",
    "        personal_scaler = StandardScaler().fit(personal_train_df)\n",
    "        scaled_personal_df = personal_scaler.transform(personal_train_df)\n",
    "        \n",
    "        hybrid_scaler = StandardScaler().fit(hybrid_train_df)\n",
    "        scaled_hybrid_df = hybrid_scaler.transform(hybrid_train_df)\n",
    "        \n",
    "        # build and predict personal model\n",
    "        personal_clf = learning_algo()\n",
    "        personal_clf.fit(scaled_personal_df, personal_train_labels)\n",
    "        personal_scaled_val_df = personal_scaler.transform(val_df)\n",
    "        personal_predictions = personal_clf.predict(personal_scaled_val_df)\n",
    "        \n",
    "        # build and predict hybrid model\n",
    "        hybrid_clf = learning_algo()\n",
    "        hybrid_clf.fit(scaled_hybrid_df, hybrid_train_labels)\n",
    "        hybrid_scaled_val_df = hybrid_scaler.transform(val_df)\n",
    "        hybrid_predictions = hybrid_clf.predict(hybrid_scaled_val_df)\n",
    "        \n",
    "        # impersonal predictions\n",
    "        impersonal_scaled_val_df = impersonal_scaler.transform(val_df)\n",
    "        impersonal_predictions = impersonal_clf.predict(impersonal_scaled_val_df)\n",
    "        \n",
    "        # validate models\n",
    "        personal_score = accuracy_score(val_labels, personal_predictions)\n",
    "        hybrid_score = accuracy_score(val_labels, hybrid_predictions)\n",
    "        impersonal_score = accuracy_score(val_labels, impersonal_predictions)\n",
    "        \n",
    "        print(\"\\tRun #%s\" % run_count)\n",
    "        print(\"\\tpersonal : %s\" % personal_score)\n",
    "        print(\"\\thybrid : %s\" % hybrid_score)\n",
    "        print(\"\\timpersonal : %s\" % impersonal_score)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        personal_row = {\"user_id\" : user_id, \n",
    "                        \"method\":\"personal\", \n",
    "                        \"run_num\" : run_count,\n",
    "                        \"training_size\" : training_size,\n",
    "                        \"accuracy\" : personal_score}\n",
    "        \n",
    "        hybrid_row = {\"user_id\" : user_id, \n",
    "                        \"method\":\"hybrid\", \n",
    "                        \"run_num\" : run_count,\n",
    "                        \"training_size\" : training_size,\n",
    "                        \"accuracy\" : hybrid_score}\n",
    "        \n",
    "        impersonal_row = {\"user_id\" : user_id, \n",
    "                        \"method\":\"impersonal\", \n",
    "                        \"run_num\" : run_count,\n",
    "                        \"training_size\" : training_size,\n",
    "                        \"accuracy\" : impersonal_score}\n",
    "        rows.append(personal_row)\n",
    "        rows.append(hybrid_row)\n",
    "        rows.append(impersonal_row)\n",
    "        \n",
    "        run_count += 1\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without class stratification\n",
    "\n",
    "command2 = '''\n",
    "rows = []\n",
    "errors = []\n",
    "training_sizes = [5,10,20,30,40,50,60]\n",
    "for user_id in user_ids:\n",
    "    for ts in training_sizes:\n",
    "        print(\"Getting scores for %s\" % user_id)\n",
    "        start = time.time()\n",
    "        try:\n",
    "            user_rows = run_experiment1(user_id, training_size=ts)\n",
    "        except ValueError as ve:\n",
    "            errors.append(ve)\n",
    "            continue\n",
    "        finish = time.time()\n",
    "        duration_in_minutes = (finish - start) / 60.\n",
    "        print(\"\\ttook %s minutes\" % (duration_in_minutes))\n",
    "        rows += user_rows\n",
    "'''\n",
    "start = time.time()\n",
    "asr = dview.execute(command2)\n",
    "print(\"finished running processes\")\n",
    "rows = dview.gather('rows')\n",
    "finish = time.time()\n",
    "print(\"Took %.3f hours\") % \n",
    "scores_df = pd.DataFrame(rows)\n",
    "scores_df.to_pickle('./scores_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with class stratification\n",
    "\n",
    "command2 = '''\n",
    "rows = []\n",
    "errors = []\n",
    "training_sizes = [5,10,20,30,40,50,60]\n",
    "for user_id in user_ids:\n",
    "    for ts in training_sizes:\n",
    "        print(\"Getting scores for %s\" % user_id)\n",
    "        start = time.time()\n",
    "        try:\n",
    "            user_rows = run_experiment1(user_id, training_size=ts, stratified=True)\n",
    "        except ValueError as ve:\n",
    "            errors.append(ve)\n",
    "            continue\n",
    "        finish = time.time()\n",
    "        duration_in_minutes = (finish - start) / 60.\n",
    "        print(\"\\ttook %s minutes\" % (duration_in_minutes))\n",
    "        rows += user_rows\n",
    "'''\n",
    "asr = dview.execute(command2)\n",
    "rows = dview.gather('rows')\n",
    "scores_df = pd.DataFrame(rows)\n",
    "scores_df.to_pickle('./scores_df_stratified.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 336\r\n",
      "-rw-rw-r--. 1 sac086 sac086 180186 Dec 13 04:24 2017-12-12_19_exp1_no_stratification.pickle\r\n",
      "-rw-rw-r--. 1 sac086 sac086 161482 Dec 13 13:18 2017-12-12_19_exp1_with_stratification.pickle\r\n",
      "drwxrwxr-x. 2 sac086 sac086     10 Dec 12 19:00 \u001b[0m\u001b[01;34mexperiment1\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ../results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load results from file\n",
    "not_stratified_scores_df = pd.read_pickle(\"../results/2017-12-12_19_exp1_no_stratification.pickle\")\n",
    "stratified_scores_df = pd.read_pickle(\"../results/2017-12-12_19_exp1_with_stratification.pickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results without Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impersonal mean scores will only include the validation set from the round where personal training data was 5 in order to have a consistent amount of trials as other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impersonal_mean_scores = []\n",
    "\n",
    "for user_id in es.user_ids:\n",
    "    user_impersonal_mean = not_stratified_scores_df[(not_stratified_scores_df['user_id'] == user_id) &\\\n",
    "                                     (not_stratified_scores_df['method'] == 'impersonal') &\\\n",
    "                                     (not_stratified_scores_df['training_size'] == 5)]['accuracy'].mean()\n",
    "    impersonal_mean_scores.append(user_impersonal_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impersonal: M=0.600, SD=0.105\n",
      "\n",
      "Training Size : 5\n",
      "\tPersonal: M=0.564, SD=0.099\n",
      "\tHybrid: M=0.620, SD=0.090\n",
      "Training Size : 10\n",
      "\tPersonal: M=0.618, SD=0.096\n",
      "\tHybrid: M=0.630, SD=0.088\n",
      "Training Size : 20\n",
      "\tPersonal: M=0.657, SD=0.086\n",
      "\tHybrid: M=0.647, SD=0.082\n",
      "Training Size : 30\n",
      "\tPersonal: M=0.679, SD=0.089\n",
      "\tHybrid: M=0.664, SD=0.071\n",
      "Training Size : 40\n",
      "\tPersonal: M=0.695, SD=0.081\n",
      "\tHybrid: M=0.670, SD=0.074\n"
     ]
    }
   ],
   "source": [
    "# Results without stratification\n",
    "print(\"Impersonal: M=%.3f, SD=%.3f\\n\" % (np.mean(impersonal_mean_scores), np.std(impersonal_mean_scores)))\n",
    "\n",
    "user_ids = not_stratified_scores_df['user_id'].unique()\n",
    "training_sizes = [5,10,20,30,40]\n",
    "\n",
    "all_personal_scores = []\n",
    "all_personal_sizes = []\n",
    "\n",
    "all_hybrid_scores = []\n",
    "all_hybrid_sizes = []\n",
    "for ts in training_sizes:\n",
    "    personal_mean_scores = []\n",
    "    hybrid_mean_scores = []\n",
    "    \n",
    "    for user_id in es.user_ids:\n",
    "        user_personal_mean = not_stratified_scores_df[(not_stratified_scores_df['user_id'] == user_id) &\\\n",
    "              (not_stratified_scores_df['method'] == 'personal') &\\\n",
    "              (not_stratified_scores_df['training_size'] == ts)]['accuracy'].mean()\n",
    "        user_hybrid_mean = not_stratified_scores_df[(not_stratified_scores_df['user_id'] == user_id) &\\\n",
    "              (not_stratified_scores_df['method'] == 'hybrid') &\\\n",
    "              (not_stratified_scores_df['training_size'] == ts)]['accuracy'].mean()\n",
    "        personal_mean_scores.append(user_personal_mean)\n",
    "        hybrid_mean_scores.append(user_hybrid_mean)\n",
    "    \n",
    "    print(\"Training Size : %s\" % ts)\n",
    "    print(\"\\tPersonal: M=%.3f, SD=%.3f\" % (np.mean(personal_mean_scores), np.std(personal_mean_scores)))\n",
    "    print(\"\\tHybrid: M=%.3f, SD=%.3f\" % (np.mean(hybrid_mean_scores), np.std(hybrid_mean_scores)))\n",
    "    \n",
    "    all_personal_scores +=  personal_mean_scores\n",
    "    all_personal_sizes += [ts] * len(personal_mean_scores)\n",
    "    \n",
    "    all_hybrid_scores += hybrid_mean_scores\n",
    "    all_hybrid_sizes += [ts] * len(hybrid_mean_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "name": "Impersonal",
         "pointpos": -1,
         "type": "box",
         "x": 0,
         "y": [
          0.604,
          0.7500000000000001,
          0.724,
          0.652,
          0.73,
          0.516,
          0.654,
          0.576,
          0.534,
          0.658,
          0.45,
          0.636,
          0.642,
          0.6060000000000001,
          0.62,
          0.6799999999999999,
          0.49800000000000005,
          0.674,
          0.612,
          0.522,
          0.6719999999999999,
          0.48,
          0.772,
          0.526,
          0.636,
          0.528,
          0.758,
          0.588,
          0.6199999999999999,
          0.784,
          0.834,
          0.43200000000000005,
          0.524,
          0.678,
          0.454,
          0.604,
          0.312,
          0.516,
          0.66,
          0.638,
          0.43,
          0.6679999999999999,
          0.40199999999999997,
          0.518,
          0.41,
          0.5159999999999999,
          0.5399999999999999,
          0.708,
          0.592,
          0.638,
          0.7040000000000001,
          0.554,
          0.6199999999999999,
          0.698,
          0.5700000000000001,
          0.5359999999999999,
          0.6359999999999999,
          0.678,
          0.504,
          0.724
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "name": "personal",
         "pointpos": -1,
         "type": "box",
         "x": [
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40
         ],
         "y": [
          0.54,
          0.6280000000000001,
          0.772,
          0.4119999999999999,
          0.6540000000000001,
          0.664,
          0.518,
          0.554,
          0.44000000000000006,
          0.47400000000000003,
          0.526,
          0.504,
          0.542,
          0.47000000000000003,
          0.504,
          0.5479999999999999,
          0.658,
          0.6199999999999999,
          0.554,
          0.466,
          0.454,
          0.546,
          0.5820000000000001,
          0.492,
          0.594,
          0.5040000000000001,
          0.502,
          0.536,
          0.5820000000000001,
          0.5040000000000001,
          0.93,
          0.5519999999999999,
          0.488,
          0.5399999999999999,
          0.514,
          0.908,
          0.7280000000000001,
          0.6079999999999999,
          0.534,
          0.5,
          0.438,
          0.548,
          0.522,
          0.5199999999999999,
          0.45600000000000007,
          0.5780000000000001,
          0.558,
          0.63,
          0.5780000000000001,
          0.546,
          0.5660000000000001,
          0.54,
          0.5720000000000001,
          0.634,
          0.6419999999999999,
          0.516,
          0.528,
          0.79,
          0.47800000000000004,
          0.524,
          0.6399999999999999,
          0.642,
          0.8100000000000002,
          0.542,
          0.618,
          0.736,
          0.612,
          0.554,
          0.466,
          0.53,
          0.462,
          0.608,
          0.54,
          0.5519999999999999,
          0.558,
          0.638,
          0.704,
          0.6180000000000001,
          0.6180000000000001,
          0.562,
          0.52,
          0.5780000000000001,
          0.726,
          0.514,
          0.608,
          0.5039999999999999,
          0.6779999999999999,
          0.604,
          0.6399999999999999,
          0.5660000000000001,
          0.96,
          0.6040000000000001,
          0.522,
          0.5960000000000001,
          0.522,
          0.924,
          0.78,
          0.7160000000000001,
          0.6319999999999999,
          0.514,
          0.602,
          0.626,
          0.544,
          0.528,
          0.594,
          0.6359999999999999,
          0.5599999999999999,
          0.6519999999999999,
          0.658,
          0.6,
          0.748,
          0.5720000000000001,
          0.5900000000000001,
          0.686,
          0.6839999999999999,
          0.562,
          0.622,
          0.752,
          0.5439999999999999,
          0.6,
          0.6799999999999999,
          0.7559999999999999,
          0.844,
          0.5720000000000001,
          0.696,
          0.78,
          0.6619999999999999,
          0.6219999999999999,
          0.532,
          0.5920000000000001,
          0.5660000000000001,
          0.6120000000000001,
          0.5960000000000001,
          0.596,
          0.6219999999999999,
          0.704,
          0.656,
          0.656,
          0.662,
          0.604,
          0.584,
          0.616,
          0.718,
          0.5740000000000001,
          0.668,
          0.5740000000000001,
          0.708,
          0.642,
          0.6140000000000001,
          0.736,
          0.954,
          0.6020000000000001,
          0.544,
          0.656,
          0.55,
          0.8960000000000001,
          0.7979999999999999,
          0.7020000000000001,
          0.646,
          0.5800000000000001,
          0.5900000000000001,
          0.6960000000000001,
          0.6379999999999999,
          0.614,
          0.612,
          0.74,
          0.554,
          0.7100000000000001,
          0.608,
          0.598,
          0.7140000000000001,
          0.716,
          0.622,
          0.734,
          0.6599999999999999,
          0.5559999999999998,
          0.6679999999999999,
          0.8020000000000002,
          0.542,
          0.6719999999999999,
          0.73,
          0.7460000000000001,
          0.768,
          0.6039999999999999,
          0.6819999999999998,
          0.7460000000000001,
          0.642,
          0.6759999999999999,
          0.526,
          0.6639999999999999,
          0.5959999999999999,
          0.6580000000000001,
          0.642,
          0.622,
          0.584,
          0.788,
          0.708,
          0.6880000000000001,
          0.646,
          0.612,
          0.514,
          0.6380000000000001,
          0.752,
          0.5980000000000001,
          0.622,
          0.608,
          0.8620000000000001,
          0.6599999999999999,
          0.672,
          0.7779999999999999,
          0.954,
          0.622,
          0.5820000000000001,
          0.582,
          0.626,
          0.908,
          0.772,
          0.764,
          0.646,
          0.6359999999999999,
          0.5900000000000001,
          0.6880000000000001,
          0.6719999999999999,
          0.694,
          0.592,
          0.708,
          0.57,
          0.688,
          0.758,
          0.6459999999999999,
          0.776,
          0.734,
          0.666,
          0.752,
          0.7200000000000001,
          0.6159999999999999,
          0.724,
          0.834,
          0.488,
          0.6799999999999999,
          0.6940000000000001,
          0.784,
          0.78,
          0.6599999999999999,
          0.764,
          0.8099999999999999,
          0.6900000000000001,
          0.6639999999999999,
          0.5519999999999999,
          0.576,
          0.604,
          0.666,
          0.6519999999999999,
          0.636,
          0.62,
          0.7360000000000001,
          0.762,
          0.6960000000000001,
          0.674,
          0.646,
          0.616,
          0.6500000000000001,
          0.7100000000000001,
          0.5640000000000001,
          0.71,
          0.61,
          0.796,
          0.68,
          0.602,
          0.76,
          0.976,
          0.682,
          0.584,
          0.6459999999999999,
          0.646,
          0.908,
          0.8019999999999999,
          0.758,
          0.674,
          0.688,
          0.6599999999999999,
          0.742,
          0.672,
          0.6699999999999999,
          0.6300000000000001,
          0.7639999999999999,
          0.644,
          0.726,
          0.7020000000000001,
          0.6719999999999999,
          0.734,
          0.68,
          0.65,
          0.778,
          0.662,
          0.716,
          0.7260000000000001,
          0.8459999999999999,
          0.54,
          0.736
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "name": "hybrid",
         "pointpos": -1,
         "type": "box",
         "x": [
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40
         ],
         "y": [
          0.6360000000000001,
          0.7500000000000001,
          0.74,
          0.6679999999999999,
          0.742,
          0.5820000000000001,
          0.6499999999999999,
          0.592,
          0.554,
          0.6399999999999999,
          0.48200000000000004,
          0.6380000000000001,
          0.6559999999999999,
          0.592,
          0.61,
          0.6519999999999999,
          0.532,
          0.7360000000000001,
          0.642,
          0.51,
          0.674,
          0.502,
          0.776,
          0.562,
          0.6519999999999999,
          0.58,
          0.8140000000000001,
          0.6040000000000001,
          0.6359999999999999,
          0.774,
          0.826,
          0.44800000000000006,
          0.624,
          0.6359999999999999,
          0.47400000000000003,
          0.624,
          0.526,
          0.55,
          0.652,
          0.64,
          0.476,
          0.682,
          0.45,
          0.5479999999999999,
          0.40800000000000003,
          0.594,
          0.58,
          0.696,
          0.62,
          0.6379999999999999,
          0.702,
          0.616,
          0.63,
          0.688,
          0.5700000000000001,
          0.546,
          0.658,
          0.6859999999999999,
          0.514,
          0.724,
          0.616,
          0.712,
          0.724,
          0.65,
          0.76,
          0.632,
          0.692,
          0.596,
          0.5440000000000002,
          0.68,
          0.45200000000000007,
          0.608,
          0.702,
          0.646,
          0.624,
          0.6839999999999999,
          0.598,
          0.744,
          0.622,
          0.458,
          0.708,
          0.518,
          0.7819999999999998,
          0.5820000000000001,
          0.654,
          0.544,
          0.8239999999999998,
          0.6559999999999999,
          0.65,
          0.7320000000000001,
          0.8400000000000001,
          0.48599999999999993,
          0.608,
          0.63,
          0.5,
          0.618,
          0.604,
          0.59,
          0.67,
          0.6459999999999999,
          0.526,
          0.6859999999999999,
          0.466,
          0.5740000000000001,
          0.45999999999999996,
          0.558,
          0.616,
          0.652,
          0.618,
          0.6740000000000002,
          0.71,
          0.6180000000000001,
          0.5700000000000001,
          0.692,
          0.5900000000000001,
          0.55,
          0.672,
          0.676,
          0.512,
          0.772,
          0.702,
          0.744,
          0.7460000000000001,
          0.6300000000000001,
          0.736,
          0.632,
          0.662,
          0.598,
          0.548,
          0.678,
          0.538,
          0.6399999999999999,
          0.6579999999999999,
          0.6040000000000001,
          0.674,
          0.71,
          0.6439999999999999,
          0.774,
          0.586,
          0.5279999999999999,
          0.642,
          0.6260000000000001,
          0.8,
          0.586,
          0.6620000000000001,
          0.584,
          0.8100000000000002,
          0.61,
          0.642,
          0.792,
          0.85,
          0.49800000000000005,
          0.584,
          0.6519999999999999,
          0.492,
          0.6060000000000001,
          0.7340000000000001,
          0.584,
          0.6900000000000001,
          0.686,
          0.538,
          0.6900000000000001,
          0.556,
          0.602,
          0.44400000000000006,
          0.72,
          0.5439999999999999,
          0.7040000000000001,
          0.618,
          0.65,
          0.694,
          0.662,
          0.61,
          0.716,
          0.622,
          0.6239999999999999,
          0.6980000000000001,
          0.704,
          0.526,
          0.724,
          0.738,
          0.752,
          0.726,
          0.6739999999999999,
          0.744,
          0.708,
          0.65,
          0.622,
          0.55,
          0.684,
          0.562,
          0.6499999999999999,
          0.644,
          0.5900000000000001,
          0.654,
          0.748,
          0.6759999999999999,
          0.7340000000000001,
          0.6479999999999999,
          0.532,
          0.638,
          0.614,
          0.77,
          0.614,
          0.664,
          0.668,
          0.818,
          0.6420000000000001,
          0.67,
          0.744,
          0.8539999999999999,
          0.5640000000000001,
          0.6,
          0.6279999999999999,
          0.532,
          0.6639999999999999,
          0.748,
          0.6340000000000001,
          0.686,
          0.6679999999999999,
          0.622,
          0.7140000000000001,
          0.614,
          0.624,
          0.458,
          0.704,
          0.606,
          0.6799999999999999,
          0.712,
          0.682,
          0.7460000000000001,
          0.65,
          0.618,
          0.7020000000000001,
          0.612,
          0.64,
          0.744,
          0.6940000000000001,
          0.5780000000000001,
          0.744,
          0.666,
          0.776,
          0.728,
          0.6799999999999999,
          0.752,
          0.72,
          0.664,
          0.678,
          0.55,
          0.662,
          0.5660000000000001,
          0.634,
          0.6759999999999999,
          0.648,
          0.6519999999999999,
          0.724,
          0.658,
          0.744,
          0.6500000000000001,
          0.532,
          0.6799999999999999,
          0.65,
          0.758,
          0.6220000000000001,
          0.6419999999999999,
          0.622,
          0.8039999999999999,
          0.696,
          0.5980000000000001,
          0.7539999999999999,
          0.8539999999999999,
          0.5740000000000001,
          0.6220000000000001,
          0.674,
          0.564,
          0.6279999999999999,
          0.796,
          0.6619999999999999,
          0.68,
          0.6819999999999999,
          0.574,
          0.708,
          0.5980000000000001,
          0.6399999999999999,
          0.466,
          0.8019999999999999,
          0.6379999999999999,
          0.696,
          0.6639999999999999,
          0.6799999999999999,
          0.732,
          0.664,
          0.6359999999999999,
          0.724,
          0.666,
          0.666,
          0.724,
          0.6880000000000001,
          0.528,
          0.806
         ]
        }
       ],
       "layout": {
        "boxmode": "group",
        "title": "Accuracy Scores By Method Without Class Stratification",
        "xaxis": {
         "title": "Amount of personal training data"
        },
        "yaxis": {
         "range": [
          0,
          1
         ],
         "title": "Accuracy"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "impersonal_trace = go.Box(y=impersonal_mean_scores,\n",
    "                          x=0,\n",
    "                          boxpoints='all',\n",
    "                          jitter=0.8,\n",
    "                          pointpos=-1,\n",
    "                          name=\"Impersonal\")\n",
    "\n",
    "\n",
    "personal_trace = go.Box(y=all_personal_scores,\n",
    "                       x=all_personal_sizes,\n",
    "                        boxpoints='all',\n",
    "                          jitter=0.8,\n",
    "                          pointpos=-1,\n",
    "                       name=\"personal\")\n",
    "\n",
    "hybrid_trace = go.Box(y=all_hybrid_scores,\n",
    "                     x=all_hybrid_sizes,\n",
    "                      boxpoints='all',\n",
    "                          jitter=0.8,\n",
    "                          pointpos=-1,\n",
    "                     name=\"hybrid\")\n",
    "\n",
    "data = [impersonal_trace, personal_trace, hybrid_trace]\n",
    "layout = go.Layout(yaxis=dict(title='Accuracy', range=[0,1]),\n",
    "                   xaxis=dict(title='Amount of personal training data'),\n",
    "                   boxmode='group',\n",
    "                   title=\"Accuracy Scores By Method Without Class Stratification\"\n",
    "                  )\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "name": "Impersonal",
         "pointpos": -1,
         "type": "box",
         "x": 0,
         "y": [
          0.604,
          0.7500000000000001,
          0.724,
          0.652,
          0.73,
          0.516,
          0.654,
          0.576,
          0.534,
          0.658,
          0.45,
          0.636,
          0.642,
          0.6060000000000001,
          0.62,
          0.6799999999999999,
          0.49800000000000005,
          0.674,
          0.612,
          0.522,
          0.6719999999999999,
          0.48,
          0.772,
          0.526,
          0.636,
          0.528,
          0.758,
          0.588,
          0.6199999999999999,
          0.784,
          0.834,
          0.43200000000000005,
          0.524,
          0.678,
          0.454,
          0.604,
          0.312,
          0.516,
          0.66,
          0.638,
          0.43,
          0.6679999999999999,
          0.40199999999999997,
          0.518,
          0.41,
          0.5159999999999999,
          0.5399999999999999,
          0.708,
          0.592,
          0.638,
          0.7040000000000001,
          0.554,
          0.6199999999999999,
          0.698,
          0.5700000000000001,
          0.5359999999999999,
          0.6359999999999999,
          0.678,
          0.504,
          0.724
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "name": "personal",
         "pointpos": -1,
         "type": "box",
         "x": [
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40
         ],
         "y": [
          0.54,
          0.6280000000000001,
          0.772,
          0.4119999999999999,
          0.6540000000000001,
          0.664,
          0.518,
          0.554,
          0.44000000000000006,
          0.47400000000000003,
          0.526,
          0.504,
          0.542,
          0.47000000000000003,
          0.504,
          0.5479999999999999,
          0.658,
          0.6199999999999999,
          0.554,
          0.466,
          0.454,
          0.546,
          0.5820000000000001,
          0.492,
          0.594,
          0.5040000000000001,
          0.502,
          0.536,
          0.5820000000000001,
          0.5040000000000001,
          0.93,
          0.5519999999999999,
          0.488,
          0.5399999999999999,
          0.514,
          0.908,
          0.7280000000000001,
          0.6079999999999999,
          0.534,
          0.5,
          0.438,
          0.548,
          0.522,
          0.5199999999999999,
          0.45600000000000007,
          0.5780000000000001,
          0.558,
          0.63,
          0.5780000000000001,
          0.546,
          0.5660000000000001,
          0.54,
          0.5720000000000001,
          0.634,
          0.6419999999999999,
          0.516,
          0.528,
          0.79,
          0.47800000000000004,
          0.524,
          0.6399999999999999,
          0.642,
          0.8100000000000002,
          0.542,
          0.618,
          0.736,
          0.612,
          0.554,
          0.466,
          0.53,
          0.462,
          0.608,
          0.54,
          0.5519999999999999,
          0.558,
          0.638,
          0.704,
          0.6180000000000001,
          0.6180000000000001,
          0.562,
          0.52,
          0.5780000000000001,
          0.726,
          0.514,
          0.608,
          0.5039999999999999,
          0.6779999999999999,
          0.604,
          0.6399999999999999,
          0.5660000000000001,
          0.96,
          0.6040000000000001,
          0.522,
          0.5960000000000001,
          0.522,
          0.924,
          0.78,
          0.7160000000000001,
          0.6319999999999999,
          0.514,
          0.602,
          0.626,
          0.544,
          0.528,
          0.594,
          0.6359999999999999,
          0.5599999999999999,
          0.6519999999999999,
          0.658,
          0.6,
          0.748,
          0.5720000000000001,
          0.5900000000000001,
          0.686,
          0.6839999999999999,
          0.562,
          0.622,
          0.752,
          0.5439999999999999,
          0.6,
          0.6799999999999999,
          0.7559999999999999,
          0.844,
          0.5720000000000001,
          0.696,
          0.78,
          0.6619999999999999,
          0.6219999999999999,
          0.532,
          0.5920000000000001,
          0.5660000000000001,
          0.6120000000000001,
          0.5960000000000001,
          0.596,
          0.6219999999999999,
          0.704,
          0.656,
          0.656,
          0.662,
          0.604,
          0.584,
          0.616,
          0.718,
          0.5740000000000001,
          0.668,
          0.5740000000000001,
          0.708,
          0.642,
          0.6140000000000001,
          0.736,
          0.954,
          0.6020000000000001,
          0.544,
          0.656,
          0.55,
          0.8960000000000001,
          0.7979999999999999,
          0.7020000000000001,
          0.646,
          0.5800000000000001,
          0.5900000000000001,
          0.6960000000000001,
          0.6379999999999999,
          0.614,
          0.612,
          0.74,
          0.554,
          0.7100000000000001,
          0.608,
          0.598,
          0.7140000000000001,
          0.716,
          0.622,
          0.734,
          0.6599999999999999,
          0.5559999999999998,
          0.6679999999999999,
          0.8020000000000002,
          0.542,
          0.6719999999999999,
          0.73,
          0.7460000000000001,
          0.768,
          0.6039999999999999,
          0.6819999999999998,
          0.7460000000000001,
          0.642,
          0.6759999999999999,
          0.526,
          0.6639999999999999,
          0.5959999999999999,
          0.6580000000000001,
          0.642,
          0.622,
          0.584,
          0.788,
          0.708,
          0.6880000000000001,
          0.646,
          0.612,
          0.514,
          0.6380000000000001,
          0.752,
          0.5980000000000001,
          0.622,
          0.608,
          0.8620000000000001,
          0.6599999999999999,
          0.672,
          0.7779999999999999,
          0.954,
          0.622,
          0.5820000000000001,
          0.582,
          0.626,
          0.908,
          0.772,
          0.764,
          0.646,
          0.6359999999999999,
          0.5900000000000001,
          0.6880000000000001,
          0.6719999999999999,
          0.694,
          0.592,
          0.708,
          0.57,
          0.688,
          0.758,
          0.6459999999999999,
          0.776,
          0.734,
          0.666,
          0.752,
          0.7200000000000001,
          0.6159999999999999,
          0.724,
          0.834,
          0.488,
          0.6799999999999999,
          0.6940000000000001,
          0.784,
          0.78,
          0.6599999999999999,
          0.764,
          0.8099999999999999,
          0.6900000000000001,
          0.6639999999999999,
          0.5519999999999999,
          0.576,
          0.604,
          0.666,
          0.6519999999999999,
          0.636,
          0.62,
          0.7360000000000001,
          0.762,
          0.6960000000000001,
          0.674,
          0.646,
          0.616,
          0.6500000000000001,
          0.7100000000000001,
          0.5640000000000001,
          0.71,
          0.61,
          0.796,
          0.68,
          0.602,
          0.76,
          0.976,
          0.682,
          0.584,
          0.6459999999999999,
          0.646,
          0.908,
          0.8019999999999999,
          0.758,
          0.674,
          0.688,
          0.6599999999999999,
          0.742,
          0.672,
          0.6699999999999999,
          0.6300000000000001,
          0.7639999999999999,
          0.644,
          0.726,
          0.7020000000000001,
          0.6719999999999999,
          0.734,
          0.68,
          0.65,
          0.778,
          0.662,
          0.716,
          0.7260000000000001,
          0.8459999999999999,
          0.54,
          0.736
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "name": "hybrid",
         "pointpos": -1,
         "type": "box",
         "x": [
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40
         ],
         "y": [
          0.6360000000000001,
          0.7500000000000001,
          0.74,
          0.6679999999999999,
          0.742,
          0.5820000000000001,
          0.6499999999999999,
          0.592,
          0.554,
          0.6399999999999999,
          0.48200000000000004,
          0.6380000000000001,
          0.6559999999999999,
          0.592,
          0.61,
          0.6519999999999999,
          0.532,
          0.7360000000000001,
          0.642,
          0.51,
          0.674,
          0.502,
          0.776,
          0.562,
          0.6519999999999999,
          0.58,
          0.8140000000000001,
          0.6040000000000001,
          0.6359999999999999,
          0.774,
          0.826,
          0.44800000000000006,
          0.624,
          0.6359999999999999,
          0.47400000000000003,
          0.624,
          0.526,
          0.55,
          0.652,
          0.64,
          0.476,
          0.682,
          0.45,
          0.5479999999999999,
          0.40800000000000003,
          0.594,
          0.58,
          0.696,
          0.62,
          0.6379999999999999,
          0.702,
          0.616,
          0.63,
          0.688,
          0.5700000000000001,
          0.546,
          0.658,
          0.6859999999999999,
          0.514,
          0.724,
          0.616,
          0.712,
          0.724,
          0.65,
          0.76,
          0.632,
          0.692,
          0.596,
          0.5440000000000002,
          0.68,
          0.45200000000000007,
          0.608,
          0.702,
          0.646,
          0.624,
          0.6839999999999999,
          0.598,
          0.744,
          0.622,
          0.458,
          0.708,
          0.518,
          0.7819999999999998,
          0.5820000000000001,
          0.654,
          0.544,
          0.8239999999999998,
          0.6559999999999999,
          0.65,
          0.7320000000000001,
          0.8400000000000001,
          0.48599999999999993,
          0.608,
          0.63,
          0.5,
          0.618,
          0.604,
          0.59,
          0.67,
          0.6459999999999999,
          0.526,
          0.6859999999999999,
          0.466,
          0.5740000000000001,
          0.45999999999999996,
          0.558,
          0.616,
          0.652,
          0.618,
          0.6740000000000002,
          0.71,
          0.6180000000000001,
          0.5700000000000001,
          0.692,
          0.5900000000000001,
          0.55,
          0.672,
          0.676,
          0.512,
          0.772,
          0.702,
          0.744,
          0.7460000000000001,
          0.6300000000000001,
          0.736,
          0.632,
          0.662,
          0.598,
          0.548,
          0.678,
          0.538,
          0.6399999999999999,
          0.6579999999999999,
          0.6040000000000001,
          0.674,
          0.71,
          0.6439999999999999,
          0.774,
          0.586,
          0.5279999999999999,
          0.642,
          0.6260000000000001,
          0.8,
          0.586,
          0.6620000000000001,
          0.584,
          0.8100000000000002,
          0.61,
          0.642,
          0.792,
          0.85,
          0.49800000000000005,
          0.584,
          0.6519999999999999,
          0.492,
          0.6060000000000001,
          0.7340000000000001,
          0.584,
          0.6900000000000001,
          0.686,
          0.538,
          0.6900000000000001,
          0.556,
          0.602,
          0.44400000000000006,
          0.72,
          0.5439999999999999,
          0.7040000000000001,
          0.618,
          0.65,
          0.694,
          0.662,
          0.61,
          0.716,
          0.622,
          0.6239999999999999,
          0.6980000000000001,
          0.704,
          0.526,
          0.724,
          0.738,
          0.752,
          0.726,
          0.6739999999999999,
          0.744,
          0.708,
          0.65,
          0.622,
          0.55,
          0.684,
          0.562,
          0.6499999999999999,
          0.644,
          0.5900000000000001,
          0.654,
          0.748,
          0.6759999999999999,
          0.7340000000000001,
          0.6479999999999999,
          0.532,
          0.638,
          0.614,
          0.77,
          0.614,
          0.664,
          0.668,
          0.818,
          0.6420000000000001,
          0.67,
          0.744,
          0.8539999999999999,
          0.5640000000000001,
          0.6,
          0.6279999999999999,
          0.532,
          0.6639999999999999,
          0.748,
          0.6340000000000001,
          0.686,
          0.6679999999999999,
          0.622,
          0.7140000000000001,
          0.614,
          0.624,
          0.458,
          0.704,
          0.606,
          0.6799999999999999,
          0.712,
          0.682,
          0.7460000000000001,
          0.65,
          0.618,
          0.7020000000000001,
          0.612,
          0.64,
          0.744,
          0.6940000000000001,
          0.5780000000000001,
          0.744,
          0.666,
          0.776,
          0.728,
          0.6799999999999999,
          0.752,
          0.72,
          0.664,
          0.678,
          0.55,
          0.662,
          0.5660000000000001,
          0.634,
          0.6759999999999999,
          0.648,
          0.6519999999999999,
          0.724,
          0.658,
          0.744,
          0.6500000000000001,
          0.532,
          0.6799999999999999,
          0.65,
          0.758,
          0.6220000000000001,
          0.6419999999999999,
          0.622,
          0.8039999999999999,
          0.696,
          0.5980000000000001,
          0.7539999999999999,
          0.8539999999999999,
          0.5740000000000001,
          0.6220000000000001,
          0.674,
          0.564,
          0.6279999999999999,
          0.796,
          0.6619999999999999,
          0.68,
          0.6819999999999999,
          0.574,
          0.708,
          0.5980000000000001,
          0.6399999999999999,
          0.466,
          0.8019999999999999,
          0.6379999999999999,
          0.696,
          0.6639999999999999,
          0.6799999999999999,
          0.732,
          0.664,
          0.6359999999999999,
          0.724,
          0.666,
          0.666,
          0.724,
          0.6880000000000001,
          0.528,
          0.806
         ]
        }
       ],
       "layout": {
        "boxmode": "group",
        "title": "Accuracy Scores By Method Without Class Stratification",
        "xaxis": {
         "title": "Amount of personal training data"
        },
        "yaxis": {
         "range": [
          0,
          1
         ],
         "title": "Accuracy"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results without stratification\n",
    "print(\"Impersonal: M=%.3f, SD=%.3f\\n\" % (np.mean(impersonal_mean_scores), np.std(impersonal_mean_scores)))\n",
    "\n",
    "user_ids = stratified_scores_df['user_id'].unique()\n",
    "training_sizes = [5,10,20,30,40]\n",
    "\n",
    "all_personal_scores = []\n",
    "all_personal_sizes = []\n",
    "\n",
    "all_hybrid_scores = []\n",
    "all_hybrid_sizes = []\n",
    "for ts in training_sizes:\n",
    "    personal_mean_scores = []\n",
    "    hybrid_mean_scores = []\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        user_personal_mean = stratified_scores_df[(stratified_scores_df['user_id'] == user_id) &\\\n",
    "              (stratified_scores_df['method'] == 'personal') &\\\n",
    "              (stratified_scores_df['training_size'] == ts)]['accuracy'].mean()\n",
    "        user_hybrid_mean = stratified_scores_df[(stratified_scores_df['user_id'] == user_id) &\\\n",
    "              (stratified_scores_df['method'] == 'hybrid') &\\\n",
    "              (stratified_scores_df['training_size'] == ts)]['accuracy'].mean()\n",
    "        personal_mean_scores.append(user_personal_mean)\n",
    "        hybrid_mean_scores.append(user_hybrid_mean)\n",
    "    \n",
    "    print(\"Training Size : %s\" % ts)\n",
    "    print(\"\\tPersonal: M=%.3f, SD=%.3f\" % (np.mean(personal_mean_scores), np.std(personal_mean_scores)))\n",
    "    print(\"\\tHybrid: M=%.3f, SD=%.3f\" % (np.mean(hybrid_mean_scores), np.std(hybrid_mean_scores)))\n",
    "    \n",
    "    all_personal_scores +=  personal_mean_scores\n",
    "    all_personal_sizes += [ts] * len(personal_mean_scores)\n",
    "    \n",
    "    all_hybrid_scores += hybrid_mean_scores\n",
    "    all_hybrid_sizes += [ts] * len(hybrid_mean_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
