{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib.machinery\n",
    "import sys\n",
    "sys.path.append('/home/sac086/extrasensory/')\n",
    "import extrasense as es\n",
    "from sklearn.metrics import accuracy_score, make_scorer, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = es.get_impersonal_data(leave_users_out=[], data_type=\"activity\", labeled_only=True)\n",
    "\n",
    "timestamps = features_df.pop('timestamp')\n",
    "label_source = features_df.pop(\"label_source\")\n",
    "labels = features_df.pop(\"label\")\n",
    "user_ids = features_df.pop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SITTING\n",
      "FIX_walking\n",
      "OR_standing\n",
      "LYING_DOWN\n",
      "FIX_running\n",
      "BICYCLING\n",
      "STAIRS\n"
     ]
    }
   ],
   "source": [
    "for l in labels.unique():\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = es.get_uids_from_es_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_baseline_model():\n",
    "    steps = []\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    steps.append(('clf', LogisticRegression(class_weight='balanced')))\n",
    "    model = Pipeline(steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_ind(test_fold_uids, all_user_ids):\n",
    "    bool_arr = all_user_ids.isin(test_fold_uids)\n",
    "    test_ind = all_user_ids.index[bool_arr]\n",
    "    bool_arr = np.logical_not(bool_arr)\n",
    "    train_ind = all_user_ids.index[bool_arr]\n",
    "    return train_ind, test_ind\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ind, test_ind = get_train_test_ind(folds[0], user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(y, y_pred, verbose=True):\n",
    "    predictions = []\n",
    "    # Naive accuracy (correct classification rate):\n",
    "    accuracy = np.mean(y_pred == y);\n",
    "    \n",
    "    # Count occorrences of true-positive, true-negative, false-positive, and false-negative:\n",
    "    tp = np.sum(np.logical_and(y_pred,y));\n",
    "    tn = np.sum(np.logical_and(np.logical_not(y_pred),np.logical_not(y)));\n",
    "    fp = np.sum(np.logical_and(y_pred,np.logical_not(y)));\n",
    "    fn = np.sum(np.logical_and(np.logical_not(y_pred),y));\n",
    "    \n",
    "    # Sensitivity (=recall=true positive rate) and Specificity (=true negative rate):\n",
    "    sensitivity = float(tp) / (tp+fn);\n",
    "    specificity = float(tn) / (tn+fp);\n",
    "    \n",
    "    # Balanced accuracy is a more fair replacement for the naive accuracy:\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2.;\n",
    "    \n",
    "    # Precision:\n",
    "    # Beware from this metric, since it may be too sensitive to rare labels.\n",
    "    # In the ExtraSensory Dataset, there is large skew among the positive and negative classes,\n",
    "    # and for each label the pos/neg ratio is different.\n",
    "    # This can cause undesirable and misleading results when averaging precision across different labels.\n",
    "    precision = float(tp) / (tp+fp);\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\"*10);\n",
    "        print('Accuracy*:         %.2f' % accuracy);\n",
    "        print('Sensitivity (TPR): %.2f' % sensitivity);\n",
    "        print('Specificity (TNR): %.2f' % specificity);\n",
    "        print('Balanced accuracy: %.2f' % balanced_accuracy);\n",
    "        print('Precision**:       %.2f' % precision);\n",
    "        print(\"-\"*10);\n",
    "        \n",
    "    return {'sensitivity' : sensitivity,\n",
    "            'specificity' : specificity,\n",
    "            'accuracy' : accuracy,\n",
    "            'balanced accuracy' : balanced_accuracy,\n",
    "            'precision' : precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model_getter, context):\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for i, kf in enumerate(folds):\n",
    "        print('Fold #%s' % i)\n",
    "        model = model_getter()\n",
    "        \n",
    "        train_ind, test_ind = get_train_test_ind(kf, user_ids)\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        X_train = features_df.iloc[train_ind]\n",
    "        y_train = labels.iloc[train_ind]\n",
    "        y_train = np.array([1 if y == context else 0 for y in y_train])\n",
    "        \n",
    "        X_test = features_df.iloc[test_ind]\n",
    "        y_test = labels.iloc[test_ind]\n",
    "        y_test = np.array([1 if y == context else 0 for y in y_test])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Testing model...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = get_metrics(y_test, y_pred, verbose=True)\n",
    "        fold_metrics.append(metrics)\n",
    "    \n",
    "    return fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #0\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.65\n",
      "Sensitivity (TPR): 0.54\n",
      "Specificity (TNR): 0.74\n",
      "Balanced accuracy: 0.64\n",
      "Precision**:       0.64\n",
      "----------\n",
      "Fold #1\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.60\n",
      "Sensitivity (TPR): 0.49\n",
      "Specificity (TNR): 0.68\n",
      "Balanced accuracy: 0.59\n",
      "Precision**:       0.56\n",
      "----------\n",
      "Fold #2\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.65\n",
      "Sensitivity (TPR): 0.55\n",
      "Specificity (TNR): 0.73\n",
      "Balanced accuracy: 0.64\n",
      "Precision**:       0.61\n",
      "----------\n",
      "Fold #3\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.68\n",
      "Sensitivity (TPR): 0.56\n",
      "Specificity (TNR): 0.75\n",
      "Balanced accuracy: 0.66\n",
      "Precision**:       0.58\n",
      "----------\n",
      "Fold #4\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.59\n",
      "Sensitivity (TPR): 0.56\n",
      "Specificity (TNR): 0.62\n",
      "Balanced accuracy: 0.59\n",
      "Precision**:       0.61\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_model(get_baseline_model, 'SITTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_metrics(metrics, verbose=True):\n",
    "    mean_metrics = {}\n",
    "    \n",
    "    for fold_metrics in metrics:\n",
    "        for key, val in fold_metrics.items():\n",
    "            if key in mean_metrics:\n",
    "                mean_metrics[key].append(val)\n",
    "            else:\n",
    "                mean_metrics[key] = [val]\n",
    "    \n",
    "    print(\"-\"*10);\n",
    "    print('Accuracy*:         %.2f' % np.mean(mean_metrics['accuracy']));\n",
    "    print('Sensitivity (TPR): %.2f' % np.mean(mean_metrics['sensitivity']));\n",
    "    print('Specificity (TNR): %.2f' % np.mean(mean_metrics['specificity']))\n",
    "    print('Balanced accuracy: %.2f' % np.mean(mean_metrics['balanced accuracy']))\n",
    "    print('Precision**:       %.2f' % np.mean(mean_metrics['precision']))\n",
    "    print(\"-\"*10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Accuracy*:         0.63\n",
      "Sensitivity (TPR): 0.54\n",
      "Specificity (TNR): 0.70\n",
      "Balanced accuracy: 0.62\n",
      "Precision**:       0.60\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "get_mean_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_standing : 37622\n",
      "STAIRS : 822\n",
      "FIX_running : 1090\n",
      "BICYCLING : 5020\n",
      "LYING_DOWN : 104208\n",
      "FIX_walking : 22135\n",
      "SITTING : 136346\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(labels)\n",
    "for key, val in c.items():\n",
    "    print(\"%s : %s\" % (key, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xgb_model():\n",
    "    steps = []\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    steps.append(('clf', xgb.XGBClassifier()))\n",
    "    model = Pipeline(steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #0\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.67\n",
      "Sensitivity (TPR): 0.64\n",
      "Specificity (TNR): 0.69\n",
      "Balanced accuracy: 0.66\n",
      "Precision**:       0.64\n",
      "----------\n",
      "Fold #1\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.65\n",
      "Sensitivity (TPR): 0.57\n",
      "Specificity (TNR): 0.72\n",
      "Balanced accuracy: 0.64\n",
      "Precision**:       0.62\n",
      "----------\n",
      "Fold #2\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.71\n",
      "Sensitivity (TPR): 0.59\n",
      "Specificity (TNR): 0.81\n",
      "Balanced accuracy: 0.70\n",
      "Precision**:       0.71\n",
      "----------\n",
      "Fold #3\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.68\n",
      "Sensitivity (TPR): 0.66\n",
      "Specificity (TNR): 0.70\n",
      "Balanced accuracy: 0.68\n",
      "Precision**:       0.58\n",
      "----------\n",
      "Fold #4\n",
      "Training model...\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.59\n",
      "Sensitivity (TPR): 0.54\n",
      "Specificity (TNR): 0.64\n",
      "Balanced accuracy: 0.59\n",
      "Precision**:       0.62\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_model(get_xgb_model, 'SITTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Accuracy*:         0.66\n",
      "Sensitivity (TPR): 0.60\n",
      "Specificity (TNR): 0.71\n",
      "Balanced accuracy: 0.66\n",
      "Precision**:       0.63\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "get_mean_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
