{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing A Stacked Ensemble Estimator That Is Scikit Learn Compatible"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scott Allen Cambo [(Website)](www.scottallencambo.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many of you know, [Scikit Learn](http://scikit-learn.org/stable/) is easily the most popular library for machine learning in Python. Like me, you were probably first exposed to it as part of a classroom project or tutorial and the excitment of seeing some reasonable accuracy on fresh new data was enough to get you on the machine learning bandwagon. Even though Scikit Learn is well designed and flexible, you will eventually find yourself saying \"If only it could do *this other thing*, then I'd really be getting somewhere\". When this happens, the first thing you should do is check to make sure that someone else hasn't already thought of this and developed exactly what you need (or at least something that is close enough). If you still can't find what you are looking for, then you may need to delve into the exciting world of extending Scikit Learn with your own custom functionalities.\n",
    "\n",
    "In this blog, I'll give an example of a scenario where I wanted to recreate an approach to classifying *context* by Vaizman, Ellis, and Lanckriet at UCSD using their **[publicly available Extrasensory dataset](http://extrasensory.ucsd.edu/)**. In this work, classifiers trained on features derived from various sensors (accelerometer, gyroscope, GPS, etc.) are combined to get better classification accuracy than they might have independently. The paper talks about this in the context of \"sensor fusion\", a topic within ubiquitous computing that aims to resolve input from multiple sensors for better insight into the surrounding environment. However, the ways of doing sensor fusion described in the paper are really two different forms of ensemble learning called ***bagging*** and ***stacking***.\n",
    "\n",
    "### Note : Scikit learn already has built-in functionality for creating bagging estimators from a set of base estimators. In practice, you should use these whenever possible.\n",
    "\n",
    "However, for this research, there are a couple additional features that will probably be helpful:\n",
    "* We want to be able to tell the estimator which exactly which sensors to use training data from\n",
    "* The ability to \"stack\" classifiers. \n",
    "\n",
    "This means that a meta classifier learns to predict the correct label using the predictions from a group of varying classifiers that are each prone to different errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore this part here. If you are curious about the code that I wrote for importing the data, you can find it in the \"extrasense\" directory of my **[GitHub repo for this project](https://github.com/scottofthescience/extrasensory_ar_analysis)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/sac086/extrasensory/')\n",
    "import extrasense as es\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "import xgboost as xgb\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = es.get_impersonal_data(leave_users_out=[], drop_nan_rows=False, sensors=None, label_type=\"activity\", labeled_only=True)\n",
    "\n",
    "timestamps = features_df.pop('timestamp')\n",
    "label_source = features_df.pop(\"label_source\")\n",
    "labels = features_df.pop(\"label\")\n",
    "user_ids = features_df.pop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BICYCLING : 5020\n",
      "FIX_running : 1090\n",
      "LYING_DOWN : 141461\n",
      "FIX_walking : 22136\n",
      "SITTING : 136728\n",
      "STAIRS : 822\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(labels)\n",
    "for key, val in c.items():\n",
    "    print(\"%s : %s\" % (key, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Extrasensory group was thoughtful enough to provide \n",
    "# the user id numbers of the participants whose data was \n",
    "# used in each fold of the cross validation process.\n",
    "# We'll use those same folds so that we can more directly \n",
    "# compare the results of our classifier with theirs.\n",
    "\n",
    "folds = es.get_uids_from_es_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've started using the [Pipeline classes](http://scikit-learn.org/stable/modules/pipeline.html) in Scikit Learn to make sure that I'm properly normalizing my data and handling NaNs. This can be an easy thing to forget and a tedious thing to add to every training process so instead I have the *make_pipeline()* function below that adds all this to my estimator of choice. For this blog, we'll be using this function with our custom ensemble estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pipeline(clf, **params):\n",
    "    \"\"\"Helper function that takes a classifier and its parameters as input\n",
    "    and returns a pipeline with an imputer that replaces NaN values with the mean value for\n",
    "    that feature and a StandardScaler() for normalizing the data to have a zero mean and unit variance.\n",
    "    \n",
    "    Args:\n",
    "        clf (sklearn estimator): the scikit learn compatible estimator that should be added to the pipeline\n",
    "        **params (dict): a dictionary representing the parameter names and values for clf\n",
    "    \n",
    "    Returns:\n",
    "        A Scikit Learn Pipeline object with clf(**params), an Imputer, and a StandardScaler\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    steps.append(('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)))\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    steps.append(('clf', clf(**params)))\n",
    "    model = Pipeline(steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an example of how this function gets used\n",
    "clf = make_pipeline(LogisticRegression, **dict(class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next three functions are used to operationalize the same cross validation metrics that\n",
    "were used in the ExtraSensory papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_ind(test_fold_uids, all_user_ids):\n",
    "    \"\"\"Takes in the test fold ids and the list of all user ids \n",
    "    for our features and returns the indeces of the features that should be\n",
    "    in the training data and test data\n",
    "    \n",
    "    Args:\n",
    "        test_fold_uids (list): list containing the user ids of those who should be in the test fold\n",
    "        all_user_ids (list): list where each element represents a sample and id of the user it came from\n",
    "    \n",
    "    Returns:\n",
    "        a tuple where the first element is a list of the traing indeces and the second element is a list\n",
    "        of the test indeces\"\"\"\n",
    "    bool_arr = all_user_ids.isin(test_fold_uids)\n",
    "    test_ind = all_user_ids.index[bool_arr]\n",
    "    bool_arr = np.logical_not(bool_arr)\n",
    "    train_ind = all_user_ids.index[bool_arr]\n",
    "    return train_ind, test_ind\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(y, y_pred, verbose=True):\n",
    "    \"\"\"This function takes the true labels and the predicted labels and returns the \n",
    "    accuracy, sensitivity, specificity, balanced accuracy, and precision that were used in\n",
    "    the ExtraSensory paper.\n",
    "    \n",
    "    Args:\n",
    "        y (list): the list of true labels\n",
    "        y_pred (list): the list of predictions\n",
    "        verbose (bool): If true, the metrics will also be printed to the output\n",
    "    \n",
    "    Returns:\n",
    "        a dictionary with the metrics and their values\"\"\"\n",
    "    predictions = []\n",
    "    # Naive accuracy (correct classification rate):\n",
    "    accuracy = np.mean(y_pred == y);\n",
    "    \n",
    "    # Count occorrences of true-positive, true-negative, false-positive, and false-negative:\n",
    "    tp = np.sum(np.logical_and(y_pred,y));\n",
    "    tn = np.sum(np.logical_and(np.logical_not(y_pred),np.logical_not(y)));\n",
    "    fp = np.sum(np.logical_and(y_pred,np.logical_not(y)));\n",
    "    fn = np.sum(np.logical_and(np.logical_not(y_pred),y));\n",
    "    \n",
    "    # Sensitivity (=recall=true positive rate) and Specificity (=true negative rate):\n",
    "    sensitivity = float(tp) / (tp+fn);\n",
    "    specificity = float(tn) / (tn+fp);\n",
    "    \n",
    "    # Balanced accuracy is a more fair replacement for the naive accuracy:\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2.;\n",
    "    \n",
    "    # Precision:\n",
    "    # Beware from this metric, since it may be too sensitive to rare labels.\n",
    "    # In the ExtraSensory Dataset, there is large skew among the positive and negative classes,\n",
    "    # and for each label the pos/neg ratio is different.\n",
    "    # This can cause undesirable and misleading results when averaging precision across different labels.\n",
    "    precision = float(tp) / (tp+fp);\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\"*10);\n",
    "        print('Accuracy*:         %.2f' % accuracy);\n",
    "        print('Sensitivity (TPR): %.2f' % sensitivity);\n",
    "        print('Specificity (TNR): %.2f' % specificity);\n",
    "        print('Balanced accuracy: %.2f' % balanced_accuracy);\n",
    "        print('Precision**:       %.2f' % precision);\n",
    "        print(\"-\"*10);\n",
    "        \n",
    "    return {'sensitivity' : sensitivity,\n",
    "            'specificity' : specificity,\n",
    "            'accuracy' : accuracy,\n",
    "            'balanced accuracy' : balanced_accuracy,\n",
    "            'precision' : precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model_getter, context, **params):\n",
    "    \"\"\"this function is used to operationalize the evaluation of an algorithm\n",
    "    so that it can be directly compared to the results of the ExtraSensory work.\n",
    "    Specifically, it uses the exact cross validation folds and metrics of the original \n",
    "    work.\n",
    "    \n",
    "    Args:\n",
    "        model_getter (function or class): this should be a function the returns an instance of the model to evaluate\n",
    "                                          or an instance of the model's class.\n",
    "        context (str): This is a string that indicates which context label to evaluate\n",
    "        \n",
    "        **params (dict): a dictionary representing the model parameter names and the values to use\n",
    "        \n",
    "    Returns:\n",
    "        a list of the metrics returned for all folds\n",
    "    \"\"\"\n",
    "    folds = es.get_uids_from_es_folds()\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for i, kf in enumerate(folds):\n",
    "        print('Fold #%s' % i)\n",
    "        model = model_getter(**params)\n",
    "        \n",
    "        train_ind, test_ind = get_train_test_ind(kf, user_ids)\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        X_train = features_df.iloc[train_ind]\n",
    "        y_train = labels.iloc[train_ind]\n",
    "        y_train = np.array([1 if y == context else 0 for y in y_train])\n",
    "        \n",
    "        X_test = features_df.iloc[test_ind]\n",
    "        y_test = labels.iloc[test_ind]\n",
    "        y_test = np.array([1 if y == context else 0 for y in y_test])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Testing model...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = get_metrics(y_test, y_pred, verbose=True)\n",
    "        fold_metrics.append(metrics)\n",
    "    \n",
    "    return fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_metrics(metrics):\n",
    "    \"\"\"This function takes the list of fold metrics and aggregates them into a single metric\n",
    "    Args:\n",
    "        metrics (list): list of metrics from the get_metrics() method \n",
    "                        representing the results of each cross validation fold\"\"\"\n",
    "    mean_metrics = {}\n",
    "    \n",
    "    for fold_metrics in metrics:\n",
    "        for key, val in fold_metrics.items():\n",
    "            if key in mean_metrics:\n",
    "                mean_metrics[key].append(val)\n",
    "            else:\n",
    "                mean_metrics[key] = [val]\n",
    "    \n",
    "    print(\"-\"*10);\n",
    "    print('Accuracy*:         %.2f' % np.mean(mean_metrics['accuracy']));\n",
    "    print('Sensitivity (TPR): %.2f' % np.mean(mean_metrics['sensitivity']));\n",
    "    print('Specificity (TNR): %.2f' % np.mean(mean_metrics['specificity']))\n",
    "    print('Balanced accuracy: %.2f' % np.mean(mean_metrics['balanced accuracy']))\n",
    "    print('Precision**:       %.2f' % np.mean(mean_metrics['precision']))\n",
    "    print(\"-\"*10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating Our Ensemble Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn provides A LOT of functionality for extending the code base, but it's important to keep in mind what it is that you actually want to achieve. A lot of this functionality is in place to help developers who are either planning to contribute to the scikit learn code base or they are planning to maintain a pretty serious development project that needs to be compatible (e.g.: Scikit Flow which is a wrapper for tensorflow that makes it compatible with Scikit Learn). If this is the direction you are thinking of then you should probably read the [\"contributing\" page](http://scikit-learn.org/stable/developers/contributing.html) on the scikit learn website and not this blog.\n",
    "\n",
    "For this work, we are simply making a somewhat maintainable, scikit learn compatible, but definitely not production code worthy estimator class. Fortunately for us, scikit learn is developed with a [\"Duck Typing\"](https://en.wikipedia.org/wiki/Duck_typing) principle meaning that if it talks like a scikit learn estimator and it walks like a scikit learn estimator, then it is, for all practicle purposes of compatibility, a scikit learn estimator. With that in mind, we need to know what scikit learn will be looking for when we hand it one of our own estimators...\n",
    "\n",
    "### Parameters\n",
    "Scikit learn methods and classes for optimizing and evaluating, like GridSearchCV, need to access and manipulate\n",
    "(nearly) all the parameters used in the estimator. It will specifically call ```get_params()``` and ```set_params()``` methods\n",
    "of the estimator to do this. Fortunately, we don't have to write these ourselves since they are inherited from \n",
    "the BaseEstimator class that we'll extend. We will need to make sure that our initializer method, ```__init__```, can take\n",
    "a dictionary of parameters as an argument and that each parameter has a default value that is set in the arguments (even if that value is ```None```).\n",
    "\n",
    "Here's an example where we use the **[```inspect```](https://docs.python.org/3/library/inspect.html)** module to set the class attributes in a clean way:\n",
    "```\n",
    "class NewClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k_neighbors=5, distance_function=None):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "```\n",
    "Generally speaking, you won't need to worry about what a frame is. All you really \n",
    "need to know is that this code is getting the argument values and names from the current method\n",
    "and so they can be passed as class attributes.\n",
    "\n",
    "### fit() and predict()\n",
    "It should be pretty clear why ```fit()``` and ```predict()``` are needed. ```fit()``` is scikit learn's name for all\n",
    "methods that start estimator training and ```predict()``` is the method that is generally used for making a prediction. In some cases, you might also add ```predict_proba()``` which returns the estimators confidence in each\n",
    "potential prediction. This is required if you plan on using sklearn's [```roc_auc_score```](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n",
    "\n",
    "### sklearn BaseEstimator and Mixins\n",
    "The ```BaseEstimator``` class provides a lot of the general functionality needed in an estimator \n",
    "like the ```get_params()``` and ```set_params()``` methods. There are a few Mixins to choose from\n",
    "depending on the type of estimator you are creating ([full list here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.base)). Since we are making a classifier, we'll just\n",
    "use the ```ClassifierMixin``` class.\n",
    "\n",
    "### ```check_estimator()```\n",
    "You should know that there is a check_estimator() function in scikit learn which can be used to \n",
    "make sure that your new estimator is compatible, but it just so happens that it doesn't play too well\n",
    "with ensembles as this [**github issue currently documents**](https://github.com/scikit-learn/scikit-learn/issues/6079#issuecomment-166990309). So in this case, we'll just test it in true Duck Type fashion by seeing if it quacks and walks like a duck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Our Ensemble Classifier\n",
    "Remember we want to add two things:\n",
    "* The ability to choose features based on the sensors they come from\n",
    "* The ability to take the ensemble group and feed it into a meta estimator (stacked ensemble)\n",
    "\n",
    "Creating the first feature is simple a matter of including ```sensors``` as an initialization parameter and writing the ```fit()``` and ```predict()``` methods to use the ```sensors``` list to figure out which features will be used to train which classifiers.\n",
    "\n",
    "In creating the second feature, we need to think about keeping the data we train our base estimators on separate from the data that we train our meta estimator on. Here's why.\n",
    "\n",
    "First, let's think about why stacked ensembles have potential. Patterns can arise from the outputs of an ensemble of classifiers where a subset of the ensemble err in a particular way with one set of input and a different subset of the ensemble err in a particular way with another set of input and so on... When we use a **\"bagged\"** ensemble, we ignore that there may be patterns and we aggregate across the outputs to get the most prominent answer. So if we have an ensemble of 5 classifiers, and 3 of them output ```True```, while 2 of them output ```False```, then the bagged ensemble estimator will predict ```True```. However, there is likely signal in this pattern from the output of the base estimators that we aren't using. Maybe one classifier is always wrong and its vote shouldn't be counted as much. The right meta estimator can learn this and effectively discount it. Maybe 3 of the 5 classifiers tend to err in the same way when confronted with a particular type of input and when this happens the error outweighs the correct classification of the other 2. Most meta estimators here should be able to catch this problem. For our meta estimator to have input, we must first learn from predictions that the base estimators make on something. If we use all of the training data to train our base estimators and then train our meta estimators by learning from predictions that the base estimators make on the training data they learned from, then we are not learning from how these estimators may behave in the real world. Instead, we are learning from their training errors which are nearly always different from test errors. This is why we need to reserve a good portion of our training data for the meta estimator to learn from.\n",
    "\n",
    "This means that our stacked ensemble's ```fit()``` method will need to follow this procedure:\n",
    "1. split the training data into training data for the base classifiers and training data for the meta classifier\n",
    "2. train the base classifiers\n",
    "3. use the base classifiers to make predictions on the training data for the meta classifier\n",
    "    * this creates a set of predictions from data the base classifiers have not yet seen allowing our meta estimator to learn the kind of prediction behavior the base classifiers will exhibit on data that is outside the training set\n",
    "4. use the base classifier predictions and the true labels on the meta classifier training set to train the meta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingStackingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    EnsembleClassifier can be used for a bagging or stacking ensemble with an estimator of the\n",
    "    programmer's choice.\n",
    "    \n",
    "    Attributes: \n",
    "        base_clf (sklearn compatible estimator): The base classifier to be used as the prototype for all base classifiers\n",
    "        base_clf_params (dict): A dictionary of parameters and their values to be used in each base classifier\n",
    "        meta_clf (sklearn compatible estimator): (stacked ensemble only) the classifier that learn from the predictions of all the base classifiers\n",
    "        meta_clf_params (dict): A dictionary of parameters and their values to be used in the meta estimator\n",
    "        confidence_threshold (float between 0 and 1): the prediction will be positive if the estimators confidence is greater than this number\n",
    "        sensors ([str]): the sensors representing the features to build classifiers for\n",
    "        verbose (bool): If True, more output will print to the screen\n",
    "        ensemble_type (\"bagging\" or \"stacking\"): determines the type of ensemble that will be used.\n",
    "        scale (bool):If True, scale stats are calculated during training. Training data and test data are scaled accordingly\n",
    "        impute (bool): If True, mean values are calculated during training. NaN values in training data and test \n",
    "            data are replaced with mean for that feature\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, base_clf=LogisticRegression(class_weight=\"balanced\"), base_clf_params=None, \\\n",
    "                       meta_clf=LogisticRegression(class_weight=\"balanced\"), meta_clf_params=None, \\\n",
    "                       confidence_threshold=0.5, \\\n",
    "                       sensors=None, verbose=True, ensemble_type='bagging', \\\n",
    "                       scale=True, impute=True):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "                \n",
    "    def setup_imputer(self, X_train):\n",
    "        self.imputer = Imputer(missing_values='NaN', strategy='mean', axis=0).fit(X_train)\n",
    "        return self.imputer.transform(X_train)\n",
    "    \n",
    "    def setup_standard_scaler(self, X_train):\n",
    "        self.scaler = StandardScaler().fit(X_train)\n",
    "        return self.scaler.transform(X_train)\n",
    "    \n",
    "    def fit(self, X_train, y_train, sensors=None):\n",
    "        \"\"\"The scikit learn compatible method for training the model\n",
    "        Args:\n",
    "            X_train (a numpy array or pandas DataFrame object): A matrix where each row \n",
    "                represents a training instance and column represents a feature set.\n",
    "            y_train (a numpy array or pandas Series object): an array where each element is a label that corresponds\n",
    "                with each row in X_train\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if sensors:\n",
    "            self.sensors = sensors\n",
    "            \n",
    "        if self.impute:\n",
    "            columns = X_train.columns\n",
    "            X_train = pd.DataFrame(self.setup_imputer(X_train), columns=columns)\n",
    "        \n",
    "        if self.scale:\n",
    "            columns = X_train.columns\n",
    "            X_train = pd.DataFrame(self.setup_standard_scaler(X_train), columns=columns)\n",
    "\n",
    "        if self.ensemble_type is 'bagging':\n",
    "            self.bag_fit(X_train, y_train)\n",
    "            \n",
    "        if self.ensemble_type is 'stacking':\n",
    "            self.stack_fit(X_train, y_train)\n",
    "    \n",
    "    def stack_fit(self, X_train, y_train):\n",
    "        \"\"\"A helper method for training a stacked ensemble\n",
    "        \n",
    "        Args:\n",
    "            X_train (pandas DataFrame object): A matrix where each row \n",
    "                represents a training instance and column represents a feature set.\n",
    "            y_train (a numpy array or pandas Series object): an array where each element is a label that corresponds\n",
    "                with each row in X_train\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # split training data\n",
    "        X_train_base, X_train_meta, y_train_base, y_train_meta = train_test_split(X_train, y_train, test_size=0.5)\n",
    "\n",
    "        # convert back to pandas DataFrame\n",
    "        X_train_base = pd.DataFrame(X_train_base, columns=X_train.columns)\n",
    "        X_train_meta = pd.DataFrame(X_train_meta, columns=X_train.columns)\n",
    "        \n",
    "        # train base classifiers\n",
    "        self.bag_fit(X_train_base, y_train_base)\n",
    "        y_pred_base = self.get_base_predictions(X_train_meta)\n",
    "\n",
    "        # train meta classifier\n",
    "        self.meta_clf.fit(y_pred_base, y_train_meta)\n",
    "\n",
    "    def  bag_fit(self, X_train, y_train):\n",
    "        \"\"\"A helper method for training the bagged ensemble\n",
    "        \n",
    "        Args:\n",
    "            X_train (a numpy array or pandas DataFrame object): A matrix where each row \n",
    "                represents a training instance and column represents a feature set.\n",
    "            y_train (a numpy array or pandas Series object): an array where each element is a label that corresponds\n",
    "                with each row in X_train\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.classifiers = {}\n",
    "        for sensor in self.sensors:\n",
    "            if self.verbose:\n",
    "                print(\"Training classifier with data from %s sensor\" % sensor)\n",
    "            X_train_sensor = self.get_features_for_sensor(X_train, sensor)\n",
    "            clf_sensor = clone(self.base_clf)\n",
    "            if self.base_clf_params is not None:\n",
    "                clf_sensor.set_params(**self.base_clf_params)\n",
    "                \n",
    "            clf_sensor.fit(X_train_sensor, y_train)\n",
    "            self.classifiers[sensor] = clf_sensor\n",
    "    \n",
    "    def get_features_for_sensor(self, X, sensor):\n",
    "        \"\"\"A helper method that maps the features to the sensors they were derived from.\n",
    "        \n",
    "        Args:\n",
    "            X (pandas DataFrame): the complete training feature set with named columns\n",
    "            sensor (str): the name of the sensor from which we derive the features for.\n",
    "        \n",
    "        Returns:\n",
    "            A pandas DataFrame with only the appropriate features relating to the sensor.\"\"\"\n",
    "        feature_cols = []\n",
    "        \n",
    "        for col in X.columns:\n",
    "            sensor_names = es.sensor_key_dict[sensor]\n",
    "            for sensor_name in sensor_names:\n",
    "                if sensor_name in col:\n",
    "                    feature_cols.append(col)\n",
    "                    break\n",
    "        \n",
    "        return X[X.columns.intersection(feature_cols)]\n",
    "    \n",
    "    def predict(self, X_test, sensors=None):\n",
    "        \"\"\"The scikit learn compatible method for predicting the label from features.\n",
    "        \n",
    "        Args:\n",
    "            X_test (pandas DataFrame): The features from which we will estimate a label for\n",
    "            sensors (list or None): if None, use the sensors list from initializing the classifier object. If list, \n",
    "                                    only use the classifiers that were trained on the features from these sensors.\n",
    "        Returns:\n",
    "            A list of probability estimates for each label.\"\"\"\n",
    "        if sensors is None:\n",
    "            sensors = self.sensors\n",
    "            \n",
    "        if self.impute:\n",
    "            columns = X_test.columns\n",
    "            X_test = pd.DataFrame(self.imputer.transform(X_test), columns=columns)\n",
    "        \n",
    "        if self.scale:\n",
    "            columns = X_test.columns\n",
    "            X_test = pd.DataFrame(self.scaler.transform(X_test), columns=columns)\n",
    "            \n",
    "        if self.ensemble_type is 'bagging':\n",
    "            return self.bag_predict(X_test)\n",
    "        \n",
    "        if self.ensemble_type is 'stacking':\n",
    "            return self.stack_predict(X_test)\n",
    "            \n",
    "    def bag_predict(self, X_test):\n",
    "        \"\"\"The prediction method for the bagging ensemble mode.\n",
    "        \n",
    "        Args:\n",
    "            X_test (pandas DataFrame): the features\n",
    "        \n",
    "        Returns:\n",
    "            A list of estimated labels for each test instance\"\"\"\n",
    "        \n",
    "        y_pred = self.get_base_predictions(X_test)\n",
    "        # average across rows\n",
    "        y_mean_pred = y_pred.mean(axis=1)\n",
    "        y_pred = y_mean_pred > self.confidence_threshold # may have to convert this from boolean to integer 1,0\n",
    "        return y_pred.astype(int)\n",
    "    \n",
    "    def stack_predict(self, X_test):\n",
    "        \"\"\"The prediction method for the stacking ensemble mode.\n",
    "        \n",
    "        Args:\n",
    "            X_test (pandas DataFrame): the features\n",
    "        Returns:\n",
    "            A list of estimated labels for each test instance\"\"\"\n",
    "        y_base_pred_probas = self.get_base_predictions(X_test)\n",
    "        y_pred = self.meta_clf.predict(y_base_pred_probas)\n",
    "        #y_meta_pred = self.meta_clf.predict_proba(y_base_pred_probas)[:,1]\n",
    "        #y_pred = y_meta_pred > self.confidence_threshold\n",
    "        return y_pred\n",
    "    \n",
    "    def get_base_predictions(self, X_test):\n",
    "        \"\"\"The helper method for making a set of ensemble predictions.\n",
    "        Args:\n",
    "            X_test (pandas DataFrame): the features\n",
    "        Returns:\n",
    "            A pandas DataFrame with columns representing each base estimator's probability estimates \n",
    "            and rows representing the test instances.\"\"\"\n",
    "        predictions_by_classifier = {}\n",
    "        \n",
    "        for sensor in self.sensors:\n",
    "            X_test_sensor = self.get_features_for_sensor(X_test, sensor)\n",
    "            predictions = self.classifiers[sensor].predict_proba(X_test_sensor)[:,1]\n",
    "            predictions_by_classifier[sensor] = pd.Series(predictions, name=sensor)\n",
    "\n",
    "        predictions_df = pd.concat([p for p in predictions_by_classifier.values()], axis=1)\n",
    "        \n",
    "        return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = [\"Acc\", \"Gyro\", \"Magnet\", \"WAcc\", \"Compass\", \"Loc\", \"Aud\", \"AP\", \"PS\", \"LF\"]\n",
    "clf = BoostingStackingClassifier(sensors=sensors, ensemble_type='stacking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "context = \"SITTING\"\n",
    "train_ind, test_ind = get_train_test_ind(folds[0], user_ids)\n",
    "\n",
    "print(\"Training model...\")\n",
    "X_train = features_df.iloc[train_ind]\n",
    "y_train = labels.iloc[train_ind]\n",
    "y_train = np.array([1 if y == context else 0 for y in y_train])\n",
    "\n",
    "X_test = features_df.iloc[test_ind]\n",
    "y_test = labels.iloc[test_ind]\n",
    "y_test = np.array([1 if y == context else 0 for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Late Fusion Averaging (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #0\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.87\n",
      "Sensitivity (TPR): 0.78\n",
      "Specificity (TNR): 0.87\n",
      "Balanced accuracy: 0.83\n",
      "Precision**:       0.04\n",
      "----------\n",
      "Fold #1\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.87\n",
      "Sensitivity (TPR): 0.80\n",
      "Specificity (TNR): 0.88\n",
      "Balanced accuracy: 0.84\n",
      "Precision**:       0.10\n",
      "----------\n",
      "Fold #2\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.89\n",
      "Sensitivity (TPR): 0.77\n",
      "Specificity (TNR): 0.90\n",
      "Balanced accuracy: 0.83\n",
      "Precision**:       0.17\n",
      "----------\n",
      "Fold #3\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.92\n",
      "Sensitivity (TPR): 0.68\n",
      "Specificity (TNR): 0.92\n",
      "Balanced accuracy: 0.80\n",
      "Precision**:       0.12\n",
      "----------\n",
      "Fold #4\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.91\n",
      "Sensitivity (TPR): 0.91\n",
      "Specificity (TNR): 0.91\n",
      "Balanced accuracy: 0.91\n",
      "Precision**:       0.09\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_model(BoostingStackingClassifier, \\\n",
    "                          'BICYCLING', \\\n",
    "                          sensors=sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Accuracy*:         0.89\n",
      "Sensitivity (TPR): 0.79\n",
      "Specificity (TNR): 0.89\n",
      "Balanced accuracy: 0.84\n",
      "Precision**:       0.10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "get_mean_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use Learned Weights (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #0\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.87\n",
      "Sensitivity (TPR): 0.80\n",
      "Specificity (TNR): 0.87\n",
      "Balanced accuracy: 0.84\n",
      "Precision**:       0.04\n",
      "----------\n",
      "Fold #1\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.86\n",
      "Sensitivity (TPR): 0.83\n",
      "Specificity (TNR): 0.86\n",
      "Balanced accuracy: 0.85\n",
      "Precision**:       0.10\n",
      "----------\n",
      "Fold #2\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.89\n",
      "Sensitivity (TPR): 0.81\n",
      "Specificity (TNR): 0.90\n",
      "Balanced accuracy: 0.85\n",
      "Precision**:       0.17\n",
      "----------\n",
      "Fold #3\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.93\n",
      "Sensitivity (TPR): 0.53\n",
      "Specificity (TNR): 0.93\n",
      "Balanced accuracy: 0.73\n",
      "Precision**:       0.11\n",
      "----------\n",
      "Fold #4\n",
      "Training model...\n",
      "Training classifier with data from Acc sensor\n",
      "Training classifier with data from Gyro sensor\n",
      "Training classifier with data from Magnet sensor\n",
      "Training classifier with data from WAcc sensor\n",
      "Training classifier with data from Compass sensor\n",
      "Training classifier with data from Loc sensor\n",
      "Training classifier with data from Aud sensor\n",
      "Training classifier with data from AP sensor\n",
      "Training classifier with data from PS sensor\n",
      "Training classifier with data from LF sensor\n",
      "Testing model...\n",
      "----------\n",
      "Accuracy*:         0.92\n",
      "Sensitivity (TPR): 0.90\n",
      "Specificity (TNR): 0.92\n",
      "Balanced accuracy: 0.91\n",
      "Precision**:       0.09\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_model(BoostingStackingClassifier, \\\n",
    "                          'BICYCLING', \\\n",
    "                          sensors=sensors, ensemble_type=\"stacking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Accuracy*:         0.89\n",
      "Sensitivity (TPR): 0.77\n",
      "Specificity (TNR): 0.89\n",
      "Balanced accuracy: 0.83\n",
      "Precision**:       0.10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "get_mean_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways of configuring ensembles that become important for a number of applications. Each which can merit a fair amount of customization of the scikit learn toolkit.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "**Active Learning**: The extent of disagreement between classifiers in the ensemble can be used as a measure of *uncertainty*. In active learning, it's common to make the assumption that samples we are uncertain about will provide the best improvement when labeled by a person.\n",
    "\n",
    "**Semi-supervised Learning**: In semi-supervised learning the goal is to take what is learned from supervised learning on a small pool of labeled samples and extend that to a large pool of unlabeled samples. The most confident predictions on the unlabeled samples are then added to the pool of labeled samples for another iteration of supervised learning. The challenge here is that when the structure that is learned is not perfect, we can be inadvertently adding incorrectly labeled samples to our labeled pool. However, with an ensemble of classifiers that each have a different perspective, we can choose only the samples who received the same prediction from each of our classifiers. Effectively, the ensemble becomes a higher quality vetting process for good labeled samples.\n",
    "\n",
    "Each of these approaches may merit specialized functionality for reasoning about the varying responses from the classifiers. Using what I've written above as a general template, you should be able to quickly and easily make sure that your new estimator passes the *duck test* for scikit learn compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : \n",
    "Scikit Learn is still working on being more pandas compatible. At the moment, you can feed a pandas Series or DataFrame to most scikit learn objects and functions without any problem, but you will generally receive the numpy equivelent in return.  In parts of this project, I create a variable to hold the parts of the DataFrame that are important to me (column names) and then transform the numpy array back afterward. There are other ways of doing this using either [sklearn-pandas](https://github.com/scikit-learn-contrib/sklearn-pandas) or with method decorators to wrap functions where this will happen. Either way just be aware of how well (or not well) pandas and sklearn play as you customize and extend sklearn objects.\n",
    "\n",
    "Cheers!\n",
    "\n",
    "**Cambo**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
